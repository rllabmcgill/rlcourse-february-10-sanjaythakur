{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLIE\n",
    "Model Free, used only state action values\n",
    "Every Visit MC\n",
    "Easily extensible to any TD method by changing value of LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the constants here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INFINITY = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User controlled variables and other hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LAMBDA as in controlling the depth of TD. Making it INFINITY tantamounts to MC, and making it 0 tantamounts to TD(0). You can make it anything in between to get TD(lambda) algorithm. Remember, decreasing it would increase bias and reducing variance. \n",
    "LAMBDA = INFINITY\n",
    "\n",
    "#Discount-Factor\n",
    "GAMMA = 0.9\n",
    "\n",
    "#All possible actions defined\n",
    "ACTION_UP = 'UP'\n",
    "ACTION_DOWN = 'DOWN'\n",
    "ACTION_LEFT = 'LEFT'\n",
    "ACTION_RIGHT = 'RIGHT'\n",
    "\n",
    "#Number of episodes to consider to evaluate each policy\n",
    "NUMBER_OF_EPISODES_PER_POLICY_EVALUATION = 1\n",
    "\n",
    "#Maximum number of iterations for convergence to the optimal policy\n",
    "MAXIMUM_NUMBER_OF_POLICY_ITERATIONS = 10\n",
    "\n",
    "#Start and end of any episode\n",
    "START_STATE = '00'\n",
    "END_STATE = '15'\n",
    "\n",
    "#Defining the EPSILON which would ensure regular exploration. Our EPSILON will decrease linearly with each iteration of a episode and will eventually fade away to 0 .\n",
    "EPSILON = 1\n",
    "\n",
    "#Maximum allowed episode length\n",
    "MAXIMUM_EPISODE_LENGTH = 100\n",
    "\n",
    "#WAIT TIME\n",
    "wait_time = 2\n",
    "\n",
    "#Defining colors for highlighting important aspects\n",
    "GREEN = lambda x: '\\x1b[32m{}\\x1b[0m'.format(x)\n",
    "BLUE = lambda x: '\\x1b[34m{}\\x1b[0m'.format(x)\n",
    "RED = lambda x: '\\x1b[31m{}\\x1b[0m'.format(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section defines the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_states = ['00', '01', '02', '03',\n",
    "          '04', '05', '06', '07',\n",
    "          '08', '09', '10', '11',\n",
    "          '12', '13', '14', '15']\n",
    "\n",
    "immediate_state_rewards = {'00': -1,'01': -1,'02': -1,'03': -1,\n",
    "                           '04': -1,'05': -1,'06': -1,'07': -1,\n",
    "                           '08': -1,'09': -1,'10': -1,'11': -1,\n",
    "                           '12': -1,'13': -1,'14': -1,'15': 0 \n",
    "                          }\n",
    "\n",
    "all_transitions =  {\n",
    "    '00': {ACTION_UP : '00', ACTION_RIGHT : '01', ACTION_DOWN: '04', ACTION_LEFT: '00'},\n",
    "    '01': {ACTION_UP : '01', ACTION_RIGHT : '02', ACTION_DOWN: '05', ACTION_LEFT: '00'},\n",
    "    '02': {ACTION_UP : '02', ACTION_RIGHT : '03', ACTION_DOWN: '06', ACTION_LEFT: '01'},\n",
    "    '03': {ACTION_UP : '03', ACTION_RIGHT : '03', ACTION_DOWN: '07', ACTION_LEFT: '02'},\n",
    "    '04': {ACTION_UP : '00', ACTION_RIGHT : '05', ACTION_DOWN: '08', ACTION_LEFT: '04'},\n",
    "    '05': {ACTION_UP : '01', ACTION_RIGHT : '06', ACTION_DOWN: '09', ACTION_LEFT: '04'},\n",
    "    '06': {ACTION_UP : '02', ACTION_RIGHT : '07', ACTION_DOWN: '10', ACTION_LEFT: '05'},\n",
    "    '07': {ACTION_UP : '03', ACTION_RIGHT : '07', ACTION_DOWN: '11', ACTION_LEFT: '06'},\n",
    "    '08': {ACTION_UP : '04', ACTION_RIGHT : '09', ACTION_DOWN: '12', ACTION_LEFT: '08'},\n",
    "    '09': {ACTION_UP : '05', ACTION_RIGHT : '10', ACTION_DOWN: '13', ACTION_LEFT: '08'},\n",
    "    '10': {ACTION_UP : '06', ACTION_RIGHT : '11', ACTION_DOWN: '14', ACTION_LEFT: '09'},\n",
    "    '11': {ACTION_UP : '07', ACTION_RIGHT : '11', ACTION_DOWN: '15', ACTION_LEFT: '10'},\n",
    "    '12': {ACTION_UP : '08', ACTION_RIGHT : '13', ACTION_DOWN: '12', ACTION_LEFT: '12'},\n",
    "    '13': {ACTION_UP : '09', ACTION_RIGHT : '14', ACTION_DOWN: '13', ACTION_LEFT: '12'},\n",
    "    '14': {ACTION_UP : '10', ACTION_RIGHT : '15', ACTION_DOWN: '14', ACTION_LEFT: '13'},\n",
    "    '15': {ACTION_UP : '15', ACTION_RIGHT : '15', ACTION_DOWN: '15', ACTION_LEFT: '15'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would tweak the following entities during the course of our quest of finding the optimal policy and state action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_state_action_value_pairs = {\n",
    "    '00': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '01': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '02': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '03': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '04': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '05': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '06': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '07': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '08': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '09': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '10': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '11': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '12': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '13': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '14': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '15': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "}\n",
    "\n",
    "# We initialize our policy. Our first iteration of policy iteration would anyways be uniformly random as we have initialized EPSILON as 1, and that is the nature of GLIE. After first policy iteration where we would be tweaking our policy greedily we'll end up having a deterministic policy.\n",
    "policy = {\n",
    "    '00': ACTION_UP,\n",
    "    '01': ACTION_UP,\n",
    "    '02': ACTION_UP,\n",
    "    '03': ACTION_UP,\n",
    "    '04': ACTION_UP,\n",
    "    '05': ACTION_UP,\n",
    "    '06': ACTION_UP,\n",
    "    '07': ACTION_UP,\n",
    "    '08': ACTION_UP,\n",
    "    '09': ACTION_UP,\n",
    "    '10': ACTION_UP,\n",
    "    '11': ACTION_UP,\n",
    "    '12': ACTION_UP,\n",
    "    '13': ACTION_UP,\n",
    "    '14': ACTION_UP,\n",
    "    '15': ACTION_UP,\n",
    "}\n",
    "\n",
    "total_state_action_visits = {\n",
    "    '00': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '01': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '02': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '03': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '04': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '05': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '06': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '07': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '08': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '09': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '10': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '11': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '12': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '13': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '14': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '15': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General-purpose functions and algorithm specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printPolicy():\n",
    "    print(\"Updated Policy\", end = '')\n",
    "    for state in all_states:\n",
    "        if (int(state) % 4) == 0:\n",
    "            print(\"\\n\")\n",
    "        print(state, \"::\", policy[state],\"\\t\", end = '')\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "def printStateActionValuePairs():\n",
    "    print(\" \\t\", ACTION_UP, \"\\t\", ACTION_RIGHT, \"\\t\", ACTION_DOWN, \"\\t\", ACTION_LEFT)\n",
    "    for state in all_states:\n",
    "        print(state, \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_UP], \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_RIGHT], \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_DOWN], \"\\t\", \"%.2f\" % all_state_action_value_pairs[state][ACTION_LEFT],)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "def printGridWorld(states_in_episode = [], actions_in_episode = []):\n",
    "    for state in all_states:\n",
    "        if (int(state) % 4) == 0:\n",
    "            print(\"\\n\")\n",
    "        if state in states_in_episode:\n",
    "            state = state.replace(state, GREEN(state))\n",
    "        print(state, \"\\t\", end = '')\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    #print('\\n', 'All Actions until this time')\n",
    "    \n",
    "    #for action in actions_in_episode:\n",
    "    #    print(action, \"\\t\", end = '')\n",
    "        \n",
    "def stateActionBasedDiscountedReturn(states_in_episode, actions_in_episode):\n",
    "    estimated_value = 0.0\n",
    "    effective_discounting = 1.0\n",
    "    for step_iterator in range(len(states_in_episode)):\n",
    "        if step_iterator > LAMBDA:\n",
    "            break\n",
    "        estimated_value = estimated_value + (effective_discounting * (immediate_state_rewards[states_in_episode[step_iterator]]))\n",
    "        effective_discounting = effective_discounting * GAMMA\n",
    "    estimated_value = estimated_value + (effective_discounting * (all_state_action_value_pairs[states_in_episode[step_iterator]][actions_in_episode[step_iterator]]))\n",
    "    return estimated_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updatePolicy():\n",
    "    for state, action_values in all_state_action_value_pairs.items():\n",
    "        highest_valued_action = ACTION_UP\n",
    "        highest_value = action_values[ACTION_UP]\n",
    "        if highest_value < action_values[ACTION_RIGHT]:\n",
    "            highest_valued_action = ACTION_RIGHT\n",
    "            highest_value = action_values[ACTION_RIGHT]\n",
    "        if highest_value < action_values[ACTION_DOWN]:\n",
    "            highest_valued_action = ACTION_DOWN\n",
    "            highest_value = action_values[ACTION_DOWN]\n",
    "        if highest_value < action_values[ACTION_LEFT]:\n",
    "            highest_valued_action = ACTION_LEFT\n",
    "            highest_value = action_values[ACTION_LEFT]\n",
    "            \n",
    "        policy[state] = highest_valued_action\n",
    "\n",
    "def chooseActionForRandomSampling():\n",
    "    random_throw = random.uniform(0, 1)\n",
    "    if random_throw < 0.25:\n",
    "        return ACTION_UP\n",
    "    elif random_throw < 0.5:\n",
    "        return ACTION_RIGHT\n",
    "    elif random_throw < 0.75:\n",
    "        return ACTION_DOWN\n",
    "    else:\n",
    "        return ACTION_LEFT\n",
    "\n",
    "def generateRandomlySampledEpisode():\n",
    "    current_state = START_STATE\n",
    "    states_in_episode = []\n",
    "    actions_in_episode = []\n",
    "    while (current_state != END_STATE) & (len(states_in_episode) < MAXIMUM_EPISODE_LENGTH):\n",
    "        states_in_episode.append(current_state)\n",
    "        action = chooseActionForRandomSampling()\n",
    "        actions_in_episode.append(action)\n",
    "        \n",
    "        #os.system('clear')\n",
    "        #printGridWorld(states_in_episode, actions_in_episode)\n",
    "        #time.sleep(wait_time)\n",
    "        \n",
    "        current_state = all_transitions.get(current_state).get(action)\n",
    "        \n",
    "    if current_state == END_STATE:\n",
    "        states_in_episode.append(END_STATE)\n",
    "        actions_in_episode.append(chooseActionForRandomSampling())\n",
    "    \n",
    "    #os.system('clear')\n",
    "    #printGridWorld(states_in_episode, actions_in_episode)\n",
    "    #time.sleep(wait_time)\n",
    "    \n",
    "    return states_in_episode, actions_in_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateGreedilySampledEpisode():\n",
    "    current_state = START_STATE\n",
    "    states_in_episode = []\n",
    "    actions_in_episode = []\n",
    "    while (current_state != END_STATE) & (len(states_in_episode) < MAXIMUM_EPISODE_LENGTH):\n",
    "        states_in_episode.append(current_state)\n",
    "        action = policy[current_state]\n",
    "        actions_in_episode.append(action)\n",
    "        \n",
    "        #os.system('clear')\n",
    "        #printGridWorld(states_in_episode, actions_in_episode)\n",
    "        #time.sleep(wait_time)\n",
    "        \n",
    "        current_state = all_transitions.get(current_state).get(action)\n",
    "    \n",
    "    if current_state == END_STATE:\n",
    "        states_in_episode.append(END_STATE)\n",
    "        actions_in_episode.append(policy[END_STATE])\n",
    "        \n",
    "    #os.system('clear')\n",
    "    #printGridWorld(states_in_episode, actions_in_episode)\n",
    "    #time.sleep(wait_time)\n",
    "        \n",
    "    return states_in_episode, actions_in_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random\n",
      " \t UP \t RIGHT \t DOWN \t LEFT\n",
      "00 \t -8.50 \t 0.00 \t -8.33 \t -8.65\n",
      "01 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "02 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "03 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "04 \t 0.00 \t -8.15 \t 0.00 \t 0.00\n",
      "05 \t 0.00 \t -4.69 \t -7.94 \t 0.00\n",
      "06 \t 0.00 \t 0.00 \t -3.40 \t 0.00\n",
      "07 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "08 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "09 \t -5.22 \t -7.71 \t 0.00 \t 0.00\n",
      "10 \t -3.44 \t -1.90 \t -7.46 \t 0.00\n",
      "11 \t 0.00 \t 0.00 \t -1.00 \t 0.00\n",
      "12 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "13 \t -5.70 \t 0.00 \t -6.50 \t 0.00\n",
      "14 \t 0.00 \t 0.00 \t 0.00 \t -7.18\n",
      "15 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "\n",
      "\n",
      "\n",
      "States \t Policy\n",
      "00 \t RIGHT\n",
      "01 \t UP\n",
      "02 \t UP\n",
      "03 \t UP\n",
      "04 \t UP\n",
      "05 \t UP\n",
      "06 \t UP\n",
      "07 \t UP\n",
      "08 \t UP\n",
      "09 \t DOWN\n",
      "10 \t LEFT\n",
      "11 \t UP\n",
      "12 \t UP\n",
      "13 \t RIGHT\n",
      "14 \t UP\n",
      "15 \t UP\n",
      "\n",
      "\n",
      "\n",
      "Greedily\n",
      " \t UP \t RIGHT \t DOWN \t LEFT\n",
      "00 \t -8.50 \t -10.00 \t -8.33 \t -8.65\n",
      "01 \t -10.00 \t 0.00 \t 0.00 \t 0.00\n",
      "02 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "03 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "04 \t 0.00 \t -8.15 \t 0.00 \t 0.00\n",
      "05 \t 0.00 \t -4.69 \t -7.94 \t 0.00\n",
      "06 \t 0.00 \t 0.00 \t -3.40 \t 0.00\n",
      "07 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "08 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "09 \t -5.22 \t -7.71 \t 0.00 \t 0.00\n",
      "10 \t -3.44 \t -1.90 \t -7.46 \t 0.00\n",
      "11 \t 0.00 \t 0.00 \t -1.00 \t 0.00\n",
      "12 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "13 \t -5.70 \t 0.00 \t -6.50 \t 0.00\n",
      "14 \t 0.00 \t 0.00 \t 0.00 \t -7.18\n",
      "15 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "\n",
      "\n",
      "\n",
      "States \t Policy\n",
      "00 \t DOWN\n",
      "01 \t RIGHT\n",
      "02 \t UP\n",
      "03 \t UP\n",
      "04 \t UP\n",
      "05 \t UP\n",
      "06 \t UP\n",
      "07 \t UP\n",
      "08 \t UP\n",
      "09 \t DOWN\n",
      "10 \t LEFT\n",
      "11 \t UP\n",
      "12 \t UP\n",
      "13 \t RIGHT\n",
      "14 \t UP\n",
      "15 \t UP\n",
      "\n",
      "\n",
      "\n",
      "Greedily\n",
      " \t UP \t RIGHT \t DOWN \t LEFT\n",
      "00 \t -8.50 \t -10.00 \t -9.97 \t -8.65\n",
      "01 \t -10.00 \t 0.00 \t 0.00 \t 0.00\n",
      "02 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "03 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "04 \t -10.00 \t -8.15 \t 0.00 \t 0.00\n",
      "05 \t 0.00 \t -4.69 \t -7.94 \t 0.00\n",
      "06 \t 0.00 \t 0.00 \t -3.40 \t 0.00\n",
      "07 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "08 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "09 \t -5.22 \t -7.71 \t 0.00 \t 0.00\n",
      "10 \t -3.44 \t -1.90 \t -7.46 \t 0.00\n",
      "11 \t 0.00 \t 0.00 \t -1.00 \t 0.00\n",
      "12 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "13 \t -5.70 \t 0.00 \t -6.50 \t 0.00\n",
      "14 \t 0.00 \t 0.00 \t 0.00 \t -7.18\n",
      "15 \t 0.00 \t 0.00 \t 0.00 \t 0.00\n",
      "\n",
      "\n",
      "\n",
      "States \t Policy\n",
      "00 \t UP\n",
      "01 \t RIGHT\n",
      "02 \t UP\n",
      "03 \t UP\n",
      "04 \t DOWN\n",
      "05 \t UP\n",
      "06 \t UP\n",
      "07 \t UP\n",
      "08 \t UP\n",
      "09 \t DOWN\n",
      "10 \t LEFT\n",
      "11 \t UP\n",
      "12 \t UP\n",
      "13 \t RIGHT\n",
      "14 \t UP\n",
      "15 \t UP\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAXIMUM_NUMBER_OF_POLICY_ITERATIONS = 40\n",
    "NUMBER_OF_EPISODES_PER_POLICY_EVALUATION = 1\n",
    "\n",
    "for policy_iterator in range(MAXIMUM_NUMBER_OF_POLICY_ITERATIONS):\n",
    "    print(\"Policy iteration number\", str(policy_iterator), \"\\n\")\n",
    "    EPSILON = (1/((0.2 * policy_iterator) + 1))\n",
    "    for episode_iterator in range(NUMBER_OF_EPISODES_PER_POLICY_EVALUATION):\n",
    "        \n",
    "        random_throw = random.uniform(0, 1)\n",
    "        if random_throw < EPSILON:\n",
    "            print(\"Generating samples randomly\")\n",
    "            states_in_episode, actions_in_episode = generateRandomlySampledEpisode()\n",
    "        else:\n",
    "            print(\"Generating samples greedily\")\n",
    "            states_in_episode, actions_in_episode = generateGreedilySampledEpisode()\n",
    "        \n",
    "        print(\"Trajectory Generated\", end = '')\n",
    "        printGridWorld(states_in_episode, actions_in_episode)\n",
    "        #print(states_in_episode)\n",
    "        #print(actions_in_episode)\n",
    "        \n",
    "        for step_number in range(len(states_in_episode)):\n",
    "            current_state = states_in_episode[step_number]\n",
    "            if current_state == END_STATE:\n",
    "                break\n",
    "            current_action = actions_in_episode[step_number]\n",
    "            \n",
    "            total_state_action_visits[current_state][current_action] = total_state_action_visits[current_state][current_action] + 1\n",
    "            estimated_discounted_return = stateActionBasedDiscountedReturn(states_in_episode[step_number:], actions_in_episode[step_number:])\n",
    "            all_state_action_value_pairs[current_state][current_action] = all_state_action_value_pairs[current_state][current_action] + (1/total_state_action_visits[current_state][current_action]) * (estimated_discounted_return - all_state_action_value_pairs[current_state][current_action])    \n",
    "    \n",
    "    updatePolicy()\n",
    "    #printStateActionValuePairs()\n",
    "    printPolicy()\n",
    "    print(\"\\n\")\n",
    "    time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
